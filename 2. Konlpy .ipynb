{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이\", \"부시게'가\", '가뿐하게', '지상파', '월화극을', '따돌리며', '6%를', '돌파했다.', '27일', '시청률', '조사회사', '닐슨', '코리아에', '따르면', '26일', '방송된', 'JTBC', '월화극', \"'눈이\", \"부시게'는\", '6.567%(전국', '유료가구', '기준)의', '시청률을', '기록했다.', '5회', '연속', '자체', '최고', '시청률을', '찍으며', '멈출', '줄', '모르는', '상승세를', '이어가고', '있다.', '동시에', '첫', '6%대', '돌파였다.', '동', '시간대', '방송된', '지상파', '3사', '월화극', 'SBS', \"'해치'\", 'KBS', '2TV', \"'동네변호사\", '조들호2:죄와', \"벌'\", 'MBC', \"'아이템'을\", '따돌리고', '우위를', '점했다.', 'tvN', \"'왕이\", '된', \"남자'(9.5%)를\", '잇는', '월화극', '전체', '2위에', '이름을', '올렸다.', \"'왕이\", '된', \"남자'의\", '경우', '종영을', '앞두고', '있기에', \"'눈이\", \"부시게'가\", '어디까지', '상승할', '수', '있을지', '주목된다.', '이날', '방송에는', '김혜자(김혜자)가', '방송', '말미', '시간을', '되돌리는', '시계를', '발견하는', '모습이', '그려졌다.', '전무송이', '이', '시계를', '차고', '있었고', '시계를', '본', '후', '눈빛이', '심하게', '흔들린', '김혜자의', '모습을', '통해', '다시금', '시간을', '되돌릴', '수', '있을지', '여부에', '관심이', '쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"C:\\Users\\nicen\\Desktop\\textdata.txt\"\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "\n",
    "    text_words = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        words = line.split() # 일반적으로 단어 구분은 띄어쓰기(공백) 기준으로 이뤄지기 때문에 띄어쓰기 기준으로 분절함.\n",
    "        text_words += words\n",
    "\n",
    "print(text_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitlines() 메소드 사용\n",
    "\n",
    "`splitlines()` 메소드는 줄바꿈을 기준으로 문자열을 여러 개로 나뉜뒤 리스트의 요소로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n"
     ]
    }
   ],
   "source": [
    "poem = \"\"\"죽는 날까지 하늘을 우러러\n",
    "한 점 부끄럼이 없기를,\n",
    "잎새에 이는 바람에도\n",
    "나는 괴로워했다.\n",
    "\"\"\"\n",
    "print(poem)\n",
    "\n",
    "poem1 = poem.splitlines()\n",
    "\n",
    "print(poem1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화\n",
    "\n",
    "단어 토큰화 전에 문서를 문장으로 분절하는 것은 영어나 한국어나 동일한 과정이다.\n",
    "\n",
    "지난 시간에서는 마침표(.)를 기준으로 문서의 문장들을 분절했으나 마침표 왜에도 문장을 구분하는 구분자들이 있다. 대표적으로, 마침표(.), 물음표(?), 느낌표(!)가 있다.\n",
    "\n",
    "그런데 이들 구분자가 때로 문장의 종결이 아닌 곳에서도 사용될 수 있기 때문에 이들 부호에 공백 문자가 연이어진 경우를 문장의 구분이 이루어지는 것으로 보는 것이 안전하다. 실제 구현에 있어서는 문장의 구분을 줄의 구분과 일치시켜서 문장의 구분이 텍스트 파일의 외현적 구조에 반영되도록 하는 것이 편리하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"C:\\Users\\user\\Desktop\\I\\ewha2\\NLP\\4주차 실습자료\\textdata.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "\n",
    "    text_sentences = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip() # 문자 양끝에 존재하는 공백과 \\n을 제거\n",
    "        sentences = line.split(\". \")   # 마침표를 기준으로 분절함.\n",
    "        text_sentences +=sentences\n",
    "\n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\"C:\\Users\\user\\Desktop\\I\\ewha2\\NLP\\4주차 실습자료\\textdata_add.txt\"\n",
    "\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    \n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\". \", \"\\n\")\n",
    "        line = line.replace(\"? \", \"?\\n\")\n",
    "        line = line.replace(\"! \", \"!\\n\")   \n",
    "        sub_sentences = line.splitlines()]\n",
    "        \n",
    "        text_sentences +=sub_sentences\n",
    "        \n",
    "print(len(text_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# 사용자 함수 작성\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\"C:\\Users\\nicen\\Desktop\\textdata_add.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  \n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규식을 이용한 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import re   # re 모듈 호출\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!])\\s+\", text.strip())\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\"C:\\Users\\nicen\\Desktop\\textdata_add.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda1\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽는', 'Verb'), ('날', 'Noun'), ('까지', 'Josa'), ('하늘', 'Noun'), ('을', 'Josa'), ('우러러', 'Noun'), ('한', 'Verb'), ('점', 'Noun'), ('부끄럼', 'Noun'), ('이', 'Josa'), ('없기를', 'Adjective'), (',', 'Punctuation'), ('잎새', 'Noun'), ('에', 'Josa'), ('이는', 'Verb'), ('바람', 'Noun'), ('에도', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('괴로워', 'Adjective'), ('했다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_pos = twitter.pos(text) # Twitter 모듈안에 있는 pos 사용자 함수를 호출  \n",
    "print(text_pos) # 리스트 내 튜플 형태(단어, 품사)로 출력 값 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('죽', 'VV'),\n",
       " ('는', 'ETD'),\n",
       " ('날', 'NNG'),\n",
       " ('까지', 'JX'),\n",
       " ('하늘', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('우', 'NNG'),\n",
       " ('러', 'NNP'),\n",
       " ('러', 'NNP'),\n",
       " ('한', 'MDN'),\n",
       " ('점', 'NNB'),\n",
       " ('부끄럼', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('없', 'VA'),\n",
       " ('기', 'ETN'),\n",
       " ('를', 'JKO'),\n",
       " (',', 'SP'),\n",
       " ('잎새', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('일', 'VV'),\n",
       " ('는', 'ETD'),\n",
       " ('바람', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('도', 'JX'),\n",
       " ('나', 'NP'),\n",
       " ('는', 'JX'),\n",
       " ('괴로워하', 'VV'),\n",
       " ('었', 'EPT'),\n",
       " ('다', 'EFN'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "kkma = Kkma()\n",
    "kkma.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['죽는', '날', '까지', '하늘', '을', '우러러', '한', '점', '부끄럼', '이', '없기를', ',', '잎새', '에', '이는', '바람', '에도', '나', '는', '괴로워', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_words = twitter.morphs(text)\n",
    "\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날', '하늘', '우러러', '점', '부끄럼', '잎새', '바람', '나']\n"
     ]
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_nouns = twitter.nouns(text)\n",
    "\n",
    "print(text_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 텍스트를 셀 내에서 직접 입력한 후 형태소분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '화', '여', '자', '대', '학', '교']\n"
     ]
    }
   ],
   "source": [
    "a= \"이 화 여 자 대 학 교\"\n",
    "a1= a.split(\" \")\n",
    "\n",
    "a2 =[]\n",
    "\n",
    "for word in a1:\n",
    "#    print(word)\n",
    "    a2.append(word)\n",
    "    \n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')], [], [('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('?', 'Punctuation')], [('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('!', 'Punctuation')], [('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation')], [('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')], [('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation')], [(\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('!', 'Punctuation')], [], [('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('?', 'Punctuation')], [('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 위에서 정의한 def split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)                 # morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "# main \n",
    "\n",
    "textdata = \"\"\"\n",
    "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다. \n",
    "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록? 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
    "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다! \n",
    "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다? 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다. \n",
    "\"\"\"\n",
    "\n",
    "twitter = Twitter()\n",
    "textdata_pos = get_pos(twitter, textdata)\n",
    "print(textdata_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 외부 텍스트 파일을 불러와 형태소분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda1\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('.', 'Punctuation')], [('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation')]], [[('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation')], [(\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('.', 'Punctuation')]], [[('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('.', 'Punctuation')], [('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]]]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 위에서 정의한 def split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)            # word와 pos가 출력 \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "# main \n",
    "\n",
    "input_file_name = r\"C:\\Users\\user\\Desktop\\I\\ewha2\\NLP\\4주차 실습자료\\textdata.txt\"\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "textdata_pos = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        words_pos = get_pos(twitter, line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        textdata_pos.append(words_pos)\n",
    "        \n",
    "print(textdata_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
