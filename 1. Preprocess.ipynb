{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Step\n",
    "#### 1. Tokenization : 문서, 문장, 단어로 분리\n",
    "#### 2. Stop word elimination\n",
    "#### 3. Normalization : 문장 부호 제거, 소문자/대문자 변환, 약어 전개\n",
    "#### 4. Stemming : 단어를 기본형으로 바꾸어준다. 복수형은 단수형으로, 과거형은 현재형으로 바꾸는 과정\n",
    "#### 4. Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\\n    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\\n    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\\n    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.\n",
    "    It is not easy to include all this information in just a few words. Start by writing a summary that includes whatever you think is important,\n",
    "    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.\n",
    "    Don't use abbreviations or citations in the abstract. It should be able to stand alone without any footnotes. Fig 1.1.1 shows below.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' one paragraph, of 100-250 words, which summarizes the purpose, methods, results and conclusions of the paper.',\n",
       " 'It is not easy to include all this information in just a few words.',\n",
       " 'Start by writing a summary that includes whatever you think is important,\\n    and then gradually prune it down to size by removing unnecessary words, while still retaini ng the necessary concepts.',\n",
       " \"Don't use abbreviations or citations in the abstract.\",\n",
       " 'It should be able to stand alone without any footnotes.',\n",
       " 'Fig 1.1.1 shows below.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'do', 'anything', '!', '(', 'Oh', ',', 'no', ')']\n",
      "['I', \"can't\", 'do', 'anything', 'Oh', 'no']\n"
     ]
    }
   ],
   "source": [
    "string = \"I can't do anything!(Oh, no)\"\n",
    "print(nltk.word_tokenize(string))  # 가장 기본적인 tokenization 함수. space 단위와 구두점(punctuation)을 기준으로 토큰화\n",
    "print(nltk.regexp_tokenize(string ,\"[\\w']+\")) # 정규표현은 텍스트를 어떻게 토큰화(Tokenize) 할건지에 대해 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'do', 'anything', '!', '(', 'Oh', ',', 'no', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', \"'\", 't', 'do', 'anything', '!(', 'Oh', ',', 'no', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer=WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cant do anythingOh no'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub('[^a-zA-Z\\s]', '', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Stop words elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords =list(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords = ['im','youre','also','even','dont','namsan','incheon']\n",
    "stopWords = set(stopWords + add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"can't\", 'anything', 'Oh']\n"
     ]
    }
   ],
   "source": [
    "words = ['I', \"can't\", 'do', 'anything', 'Oh', 'no']\n",
    "print([word for word in words if word not in stopWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n",
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())\n",
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자, 부호 삭제 방법 (1): replace() 메소드 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She go She go.\n",
      "She go She go\n"
     ]
    }
   ],
   "source": [
    "text = \"She go? She go.\"\n",
    "\n",
    "text_r = text.replace(\"?\", \"\")\n",
    "print(text_r)\n",
    "\n",
    "text_r1 = text_r.replace(\".\", \"\")\n",
    "print(text_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자, 부호 삭제 방법 (2): 정규식(re) 모듈 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re    # 정규식 사용 모듈 re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]\n",
    "\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "\n",
    "    for token in review: \n",
    "        new_token = x.sub(u'', token)\n",
    "        \n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 약어 전개 방법: (1) replace() 메소드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She must've gone to the market but she diddid not go\n"
     ]
    }
   ],
   "source": [
    "text = \"She must've gone to the market but she didn't go\"\n",
    "\n",
    "text_r = text.replace(\"t've\", \"t have\")\n",
    "text_r = text.replace(\"n't\", \"did not\") # 왜 여러개의 패턴을 모듈로 생성해 사용하는 것이 필요한지 예시\n",
    "\n",
    "print(text_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 약어 전개 방법: (2) 기존에 특정 패턴을 지정해 모듈로 생성하여 재사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replacers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d83f1be29d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreplacers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpReplacer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreplacer\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mRegexpReplacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplacer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"She must've gone to the market but she didn't go\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'replacers'"
     ]
    }
   ],
   "source": [
    "from replacers import RegexpReplacer\n",
    "\n",
    "replacer= RegexpReplacer()\n",
    "print(replacer.replace(\"She must've gone to the market but she didn't go\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "\t(r'won\\'t', 'will not'),\n",
    "\t(r'can\\'t', 'cannot'),\n",
    "\t(r'i\\'m', 'i am'),\n",
    "\t(r'ain\\'t', 'is not'),\n",
    "\t(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "\t(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "\t(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "\t(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "\t(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Pos name #############\n",
    "\n",
    "# CC coordinating conjunction / CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: “there is” … think of it like “there exists”)\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "### JJ adjective ‘big’/ JJR adjective, comparative ‘bigger’/ JJS adjective, superlative ‘biggest’\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "### NN noun, singular ‘desk’NNS noun plural ‘desks’/ NNP proper noun, singular ‘Harrison’/ NNPS proper noun, plural ‘Americans’\n",
    "# PDT predeterminer ‘all the kids’\n",
    "# POS possessive ending parent’s\n",
    "# PRP personal pronoun I, he, she / PRP$ possessive pronoun my, his, hers\n",
    "### RB adverb very, silently / RBR adverb, comparative better / RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO, to go ‘to’ the store.\n",
    "# UH interjection, errrrrrrrm\n",
    "### VB verb, base form take / VBD verb, past tense took / VBG verb, gerund present participle taking / VBN verb, past participle taken / VBP verb, sing. present, non-3d take / VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which / WP wh-pronoun who, what / WP$ possessive wh-pronoun whose / WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사, 형용사, 동사, 부사 lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def njvr_lemmantizer(sent):\n",
    "    global lemm\n",
    "    \n",
    "    lemm_sent = []\n",
    "    for word_pos in sent:\n",
    "        word, pos = word_pos\n",
    "        if pos[0] == 'N':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='n').lower() +'_N')\n",
    "        elif pos[0] == 'J':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='a').lower() +'_J')\n",
    "        elif pos[0] == 'V':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='v').lower() +'_V')\n",
    "        elif pos[0] == 'R':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='r').lower() +'_R')\n",
    "        else:pass\n",
    "    return lemm_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def njvr_lemmantizer2(sent):\n",
    "    global lemm\n",
    "    \n",
    "    lemm_sent = []\n",
    "    for word_pos in sent:\n",
    "        word, pos = word_pos\n",
    "        if pos[0] == 'N':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='n').lower())\n",
    "        elif pos[0] == 'J':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='a').lower())\n",
    "        elif pos[0] == 'V':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='v').lower())\n",
    "        elif pos[0] == 'R':\n",
    "            lemm_sent.append(lemm.lemmatize(word,pos='r').lower())\n",
    "        else:pass\n",
    "    return lemm_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friends', 'NNS'),\n",
       " ('wonderful', 'JJ'),\n",
       " ('stay', 'VBP'),\n",
       " ('sun', 'JJ'),\n",
       " (\"kyung's\", 'NN'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_en[\"10039364\"][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend_N', 'wonderful_J', 'stay_V', 'sun_J', \"kyung's_N\", 'place_N']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "njvr_lemmantizer(review_en[\"10039364\"][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_en2 = review_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30569/30569 [00:58<00:00, 520.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for listing_id, rev_lst in tqdm.tqdm(review_en.items()):\n",
    "    for rev in rev_lst:\n",
    "        rev_tmp = []\n",
    "        for sent in rev[1]:\n",
    "            rev_tmp.append(njvr_lemmantizer(sent))\n",
    "        rev[1] = rev_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/30569 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ed447ca59278>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mrev_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mrev_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnjvr_lemmantizer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mrev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrev_tmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-65c30e145bc6>\u001b[0m in \u001b[0;36mnjvr_lemmantizer2\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlemm_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword_pos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'N'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mlemm_sent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for listing_id, rev_lst in tqdm.tqdm(review_en2.items()):\n",
    "    for rev in rev_lst:\n",
    "        rev_tmp = []\n",
    "        for sent in rev[1]:\n",
    "            rev_tmp.append(njvr_lemmantizer2(sent))\n",
    "        rev[1] = rev_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend_N', 'wonderful_J', 'stay_V', 'sun_J', \"kyung's_N\", 'place_N']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_en[\"10039364\"][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friends', 'NNS'),\n",
       " ('wonderful', 'JJ'),\n",
       " ('stay', 'VBP'),\n",
       " ('sun', 'JJ'),\n",
       " (\"kyung's\", 'NN'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_en2[\"10039364\"][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
